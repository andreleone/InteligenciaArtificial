{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas \n",
    "import os  #Interação com o sistema operacional\n",
    "import numpy as np  #Operações numéricas e matrizes\n",
    "import cv2  #Processamento de imagem\n",
    "from tqdm import tqdm  #Barra de progresso\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  #Aumento de imagem\n",
    "from sklearn.model_selection import train_test_split  # Divisão de dados em treino e teste\n",
    "from tensorflow.keras.applications import VGG16  #Arquitetura de rede neural\n",
    "from tensorflow.keras.models import Sequential, load_model  #Construção e carregamento de modelos\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D  #Camadas da rede neural\n",
    "from tensorflow.keras.optimizers import Adam  #Otimizador\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  #Callbacks para treinamento\n",
    "from sklearn.preprocessing import LabelEncoder  #Codificação de rótulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo paths das pastas\n",
    "# Caminhos das pastas com os dados. Especifica a variável base_path como o diretório principal.\n",
    "base_path = 'C:/Users/andre/OneDrive/Desktop/Fiap/Kaggle/Chest_X-Ray'\n",
    "train_dir = os.path.join(base_path, 'treino')\n",
    "test_dir = os.path.join(base_path, 'teste')\n",
    "val_dir = os.path.join(base_path, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de preprocessamento\n",
    "image_size = 224  # Tamanho das imagens\n",
    "batch_size = 32\n",
    "\n",
    "# image_size = 224: isso significa que todas as imagens serão redimensionadas para 224x224 pixels. \n",
    "# Independentemente do tamanho original da imagem, ela será adaptada para essa dimensão durante o pré-processamento.\n",
    "# O processo é feito para garantir consistência no tamanho das imagens, o que facilita o \n",
    "# treinamento do modelo e garante que todas as entradas tenham as mesmas dimensões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais sobre o batch_size: \n",
    "Memória e Eficiência Computacional:\n",
    "- Processar um lote de 32 imagens é muito mais eficiente em termos de uso de memória do que processar todas as imagens de uma vez.\n",
    "- A memória da GPU (ou CPU) não é ilimitada, então usar batches permite que grandes conjuntos de dados sejam processados de forma mais eficiente.\n",
    "\n",
    "Treinamento Mais Estável:\n",
    "- Atualizar os parâmetros do modelo (pesos da rede neural) após cada imagem poderia levar a uma convergência instável devido à alta variação.\n",
    "- Usar batches ajuda a estabilizar o processo de treinamento, pois as atualizações são feitas com base na média de erros de várias imagens, suavizando as variações.\n",
    "\n",
    "Velocidade de Treinamento:\n",
    "- O uso de batches pode acelerar o treinamento. As operações matriciais (como as multiplicações de matrizes) são mais eficientes quando processadas em paralelo para um lote de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar as imagens\n",
    "# carrega todas as imagens de diferentes categorias (subpastas), \n",
    "# redimensiona as imagens, e cria listas de imagens e seus respectivos rótulos\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(folder):\n",
    "        class_path = os.path.join(folder, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_name in tqdm(os.listdir(class_path)):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Carregar como escala de cinza\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (image_size, image_size))\n",
    "                    images.append(np.expand_dims(img, axis=-1))  # Expandir dimensões para escala de cinza\n",
    "                    labels.append(class_name)\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trecho de código define uma função chamada `load_images_from_folder`, que tem como objetivo carregar imagens de diferentes categorias (subpastas) a partir de uma pasta principal. A função realiza as seguintes etapas:\n",
    "\n",
    "1. Inicializa duas listas vazias: `images` para armazenar as imagens e `labels` para armazenar os rótulos das categorias.\n",
    "2. Percorre todas as subpastas dentro da pasta principal especificada.\n",
    "3. Dentro de cada subpasta, itera sobre todos os arquivos de imagem.\n",
    "4. Carrega cada imagem como escala de cinza usando a biblioteca OpenCV.\n",
    "5. Redimensiona a imagem para um tamanho específico (`image_size` x `image_size`).\n",
    "6. Adiciona a imagem redimensionada à lista `images`, expandindo suas dimensões para manter o formato de escala de cinza.\n",
    "7. Adiciona o nome da subpasta (categoria) à lista `labels`.\n",
    "\n",
    "Retorna duas arrays NumPy: uma contendo todas as imagens e outra contendo seus respectivos rótulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1341/1341 [00:24<00:00, 55.28it/s]\n",
      "100%|██████████| 3875/3875 [00:27<00:00, 140.15it/s]\n",
      "100%|██████████| 234/234 [00:02<00:00, 78.38it/s] \n",
      "100%|██████████| 390/390 [00:02<00:00, 154.87it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 100.78it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 249.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# Carregar imagens de treino, teste e validação\n",
    "# utiliza a função `load_images_from_folder` para processar as imagens em suas respectivas pastas (treino, teste e validação) \n",
    "# e retorna arrays contendo as imagens e seus rótulos. \n",
    "# Esses conjuntos de dados serão usados para treinar, testar e validar o modelo\n",
    "\n",
    "train_images, train_labels = load_images_from_folder(train_dir)\n",
    "test_images, test_labels = load_images_from_folder(test_dir)\n",
    "val_images, val_labels = load_images_from_folder(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização das imagens\n",
    "# Esse trecho de código normaliza as imagens dividindo cada valor de pixel por 255. \n",
    "# Isso transforma os valores dos pixels de 0-255 para uma faixa entre 0 e 1, \n",
    "# o que ajuda a melhorar o desempenho durante o treinamento e garante que todos os pixels tenham valores na mesma faixa.\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "val_images = val_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar os rótulos\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LabelEncoder: Utiliza um codificador de rótulos para transformar rótulos de texto em valores numéricos. \n",
    "1. Inicializa um objeto `LabelEncoder`.\n",
    "2. Codifica os rótulos de treino (`train_labels`) transformando-os em valores numéricos e armazena-os em `train_labels_encoded`.\n",
    "3. Codifica os rótulos de validação (`val_labels`) utilizando a mesma transformação e armazena-os em `val_labels_encoded`.\n",
    "A codificação dos rótulos é importante para que o modelo de aprendizado de máquina possa trabalhar com os rótulos de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o ImageDataGenerator para os dados de treinamento\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalização das imagens\n",
    "    rotation_range=20,  # Rotação\n",
    "    width_shift_range=0.2,  # Deslocamento horizontal\n",
    "    height_shift_range=0.2,  # Deslocamento vertical\n",
    "    shear_range=0.2,  # Cisalhamento\n",
    "    zoom_range=0.2,  # Zoom\n",
    "    horizontal_flip=True,  # Flip horizontal\n",
    "    fill_mode='nearest'  # Preenchimento\n",
    ")\n",
    "\n",
    "# Inicializando o ImageDataGenerator para os dados de validação\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Normalização das imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `ImageDataGenerator` é uma classe da biblioteca Keras utilizada para pré-processar e gerar lotes de dados de imagem em tempo real, com aumento de dados (data augmentation).\n",
    "\n",
    "Funções principais:\n",
    "- Normalização de imagens: Converte valores de pixels de 0-255 para 0-1.\n",
    "- Aumento de dados: Aplica transformações como rotação, deslocamento, cisalhamento, zoom e flip nas imagens de treino.\n",
    "- Geração de lotes: Cria lotes de dados em tempo real durante o treinamento.\n",
    "\n",
    "Benefícios:\n",
    "- Melhora a generalização do modelo.\n",
    "- Uso eficiente da memória.\n",
    "- Reduz overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do ImageDataGenerator com rótulos codificados\n",
    "train_generator = datagen.flow(\n",
    "    np.array(train_images),  # Garantir que a forma seja (5216, 224, 224, 1)\n",
    "    train_labels_encoded,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow(\n",
    "    np.array(val_images),  # Garantir que a forma seja (624, 224, 224, 1)\n",
    "    val_labels_encoded,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configura dois objetos ImageDataGenerator para os dados de treinamento e validação com rótulos codificados:\n",
    "   - `train_generator`: Gera lotes de dados de treinamento aumentados a partir das imagens e rótulos codificados.\n",
    "   - `val_generator`: Gera lotes de dados de validação a partir das imagens e rótulos codificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carregar o modelo VGG16 pré-treinado sem a camada de saída\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega o modelo VGG16 pré-treinado sem a camada de saída:\n",
    "   - `base_model`: Carrega o modelo VGG16 com pesos pré-treinados no ImageNet e exclui a camada de saída, especificando a entrada como (224, 224, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Construir o modelo sobre o VGG16\n",
    "model = Sequential([\n",
    "    Conv2D(3, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 1)),  # Primeira camada para adaptar escala de cinza para RGB\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Congelar as camadas do modelo base VGG16\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Definindo os callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Constrói um modelo usando a base do VGG16:\n",
    "   - Adiciona uma camada Conv2D para adaptar imagens em escala de cinza para RGB.\n",
    "   - Adiciona o modelo base VGG16 pré-treinado.\n",
    "   - Adiciona camadas Flatten, Dense, Dropout e uma camada de saída com ativação sigmoide.\n",
    "\n",
    "2. Congela as camadas do modelo base VGG16 para que seus pesos não sejam atualizados durante o treinamento.\n",
    "\n",
    "3. Compila o modelo:\n",
    "   - Usa o otimizador Adam com uma taxa de aprendizado de 0.0001.\n",
    "   - Define a perda como 'binary_crossentropy'.\n",
    "   - Usa a métrica 'accuracy'.\n",
    "\n",
    "4. Define os callbacks:\n",
    "   - `early_stopping`: Para o treinamento antecipadamente se a perda de validação não melhorar após 10 épocas e restaura os melhores pesos.\n",
    "   - `reduce_lr`: Reduz a taxa de aprendizado em um fator de 0.2 se a perda de validação não melhorar após 5 épocas, com uma taxa mínima de aprendizado de 0.00001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3176s\u001b[0m 19s/step - accuracy: 0.7646 - loss: 0.4886 - val_accuracy: 0.6875 - val_loss: 0.5592 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13866s\u001b[0m 85s/step - accuracy: 0.8689 - loss: 0.2943 - val_accuracy: 0.6875 - val_loss: 0.5462 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6784s\u001b[0m 42s/step - accuracy: 0.8801 - loss: 0.2743 - val_accuracy: 0.6875 - val_loss: 0.6876 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3580s\u001b[0m 22s/step - accuracy: 0.8850 - loss: 0.2569 - val_accuracy: 0.7500 - val_loss: 0.4475 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2165s\u001b[0m 13s/step - accuracy: 0.8954 - loss: 0.2454 - val_accuracy: 0.7500 - val_loss: 0.4413 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2360s\u001b[0m 15s/step - accuracy: 0.8993 - loss: 0.2300 - val_accuracy: 0.8125 - val_loss: 0.4343 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1872s\u001b[0m 11s/step - accuracy: 0.9112 - loss: 0.2102 - val_accuracy: 0.6875 - val_loss: 0.5833 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3651s\u001b[0m 22s/step - accuracy: 0.9192 - loss: 0.1958 - val_accuracy: 0.8125 - val_loss: 0.4284 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4023s\u001b[0m 25s/step - accuracy: 0.9196 - loss: 0.2027 - val_accuracy: 0.6875 - val_loss: 0.6850 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5048s\u001b[0m 31s/step - accuracy: 0.9208 - loss: 0.1991 - val_accuracy: 0.6875 - val_loss: 0.7479 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32576s\u001b[0m 201s/step - accuracy: 0.9282 - loss: 0.1817 - val_accuracy: 0.8125 - val_loss: 0.4556 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2454s\u001b[0m 15s/step - accuracy: 0.9292 - loss: 0.1841 - val_accuracy: 0.8125 - val_loss: 0.4532 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2875s\u001b[0m 18s/step - accuracy: 0.9309 - loss: 0.1824 - val_accuracy: 0.6875 - val_loss: 0.6160 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3270s\u001b[0m 20s/step - accuracy: 0.9393 - loss: 0.1590 - val_accuracy: 0.7500 - val_loss: 0.4950 - learning_rate: 2.0000e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2537s\u001b[0m 16s/step - accuracy: 0.9393 - loss: 0.1572 - val_accuracy: 0.6875 - val_loss: 0.5051 - learning_rate: 2.0000e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2232s\u001b[0m 14s/step - accuracy: 0.9380 - loss: 0.1635 - val_accuracy: 0.8125 - val_loss: 0.4369 - learning_rate: 2.0000e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2854s\u001b[0m 18s/step - accuracy: 0.9376 - loss: 0.1625 - val_accuracy: 0.8125 - val_loss: 0.4203 - learning_rate: 2.0000e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8294s\u001b[0m 51s/step - accuracy: 0.9407 - loss: 0.1604 - val_accuracy: 0.8125 - val_loss: 0.4617 - learning_rate: 2.0000e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5236s\u001b[0m 32s/step - accuracy: 0.9421 - loss: 0.1487 - val_accuracy: 0.8750 - val_loss: 0.4122 - learning_rate: 2.0000e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4375s\u001b[0m 27s/step - accuracy: 0.9483 - loss: 0.1457 - val_accuracy: 0.8750 - val_loss: 0.4197 - learning_rate: 2.0000e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5711s\u001b[0m 35s/step - accuracy: 0.9418 - loss: 0.1522 - val_accuracy: 0.8125 - val_loss: 0.4676 - learning_rate: 2.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2790s\u001b[0m 17s/step - accuracy: 0.9364 - loss: 0.1685 - val_accuracy: 0.7500 - val_loss: 0.4699 - learning_rate: 2.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2480s\u001b[0m 15s/step - accuracy: 0.9464 - loss: 0.1455 - val_accuracy: 0.8125 - val_loss: 0.4491 - learning_rate: 2.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3140s\u001b[0m 19s/step - accuracy: 0.9367 - loss: 0.1660 - val_accuracy: 0.8750 - val_loss: 0.3973 - learning_rate: 2.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2609s\u001b[0m 16s/step - accuracy: 0.9428 - loss: 0.1506 - val_accuracy: 0.8750 - val_loss: 0.3514 - learning_rate: 2.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6678s\u001b[0m 41s/step - accuracy: 0.9467 - loss: 0.1383 - val_accuracy: 0.8750 - val_loss: 0.3473 - learning_rate: 2.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3955s\u001b[0m 24s/step - accuracy: 0.9404 - loss: 0.1481 - val_accuracy: 0.8750 - val_loss: 0.3400 - learning_rate: 2.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3660s\u001b[0m 22s/step - accuracy: 0.9456 - loss: 0.1454 - val_accuracy: 0.8125 - val_loss: 0.4162 - learning_rate: 2.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3828s\u001b[0m 24s/step - accuracy: 0.9445 - loss: 0.1475 - val_accuracy: 0.7500 - val_loss: 0.4856 - learning_rate: 2.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3673s\u001b[0m 23s/step - accuracy: 0.9394 - loss: 0.1509 - val_accuracy: 0.8750 - val_loss: 0.4055 - learning_rate: 2.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3614s\u001b[0m 22s/step - accuracy: 0.9482 - loss: 0.1367 - val_accuracy: 0.7500 - val_loss: 0.4643 - learning_rate: 2.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3425s\u001b[0m 21s/step - accuracy: 0.9484 - loss: 0.1496 - val_accuracy: 0.6875 - val_loss: 0.5435 - learning_rate: 2.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5305s\u001b[0m 33s/step - accuracy: 0.9463 - loss: 0.1433 - val_accuracy: 0.7500 - val_loss: 0.4431 - learning_rate: 1.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4506s\u001b[0m 28s/step - accuracy: 0.9448 - loss: 0.1523 - val_accuracy: 0.7500 - val_loss: 0.4730 - learning_rate: 1.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3563s\u001b[0m 22s/step - accuracy: 0.9424 - loss: 0.1510 - val_accuracy: 0.8125 - val_loss: 0.4119 - learning_rate: 1.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3462s\u001b[0m 21s/step - accuracy: 0.9499 - loss: 0.1318 - val_accuracy: 0.8750 - val_loss: 0.3998 - learning_rate: 1.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m163/163\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3449s\u001b[0m 21s/step - accuracy: 0.9520 - loss: 0.1305 - val_accuracy: 0.7500 - val_loss: 0.4696 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina o modelo com os dados de treinamento e validação.\n",
    "1. Chama o método `fit` do modelo para iniciar o treinamento.\n",
    "2. Usa o `train_generator` para fornecer lotes de dados de treinamento.\n",
    "3. Define o número de épocas de treinamento como 50.\n",
    "4. Usa o `val_generator` para fornecer dados de validação.\n",
    "5. Adiciona os callbacks `early_stopping` e `reduce_lr` para monitorar a perda de validação e ajustar a taxa de aprendizado durante o treinamento.\n",
    "6. Armazena o histórico do treinamento na variável `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Salvar o modelo\n",
    "model.save('modelo_pulmoes_vgg16_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva o modelo treinado em um arquivo H5.\n",
    "1. Chama o método `save` do modelo.\n",
    "2. Salva o modelo treinado no arquivo 'modelo_pulmoes_vgg16_2.h5'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [00:12<00:00, 19.38it/s]\n",
      "100%|██████████| 390/390 [00:08<00:00, 43.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 9s/step - accuracy: 0.8713 - loss: 0.3638\n",
      "Test Loss: 0.2635897696018219\n",
      "Test Accuracy: 0.9054487347602844\n"
     ]
    }
   ],
   "source": [
    "# Carregar imagens de teste\n",
    "test_images, test_labels = load_images_from_folder(test_dir)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalizar as imagens de teste\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Avaliar o modelo\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels_encoded, batch_size=batch_size)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avalia o modelo com as imagens de teste:\n",
    "     A função `model.evaluate` é usada para avaliar o desempenho de um modelo treinado em um conjunto de dados de teste. Ela calcula a perda (loss) e as métricas definidas (como precisão) para os dados de entrada fornecidos.\n",
    " - `test_loss, test_accuracy = model.evaluate(test_images, test_labels_encoded, batch_size=batch_size)`: Avalia a perda e a precisão do modelo nos dados de teste.\n",
    "   - `print(f'Test Loss: {test_loss}')`: Imprime a perda do teste.\n",
    "   - `print(f'Test Accuracy: {test_accuracy}')`: Imprime a precisão do teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.8750 - loss: 0.3400\n",
      "Val Loss: 0.33996444940567017\n",
      "Val Accuracy: 0.875\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 5s/step - accuracy: 0.8713 - loss: 0.3638\n",
      "Test Loss: 0.2635897696018219\n",
      "Test Accuracy: 0.9054487347602844\n",
      "\n",
      "### Análise dos Resultados ###\n",
      "Training Accuracy: 95% (estimado)\n",
      "Training Loss: ~0.20 (estimado)\n",
      "Val Accuracy: 0.875\n",
      "Val Loss: 0.33996444940567017\n",
      "Test Accuracy: 0.9054487347602844\n",
      "Test Loss: 0.2635897696018219\n",
      "Conclusão: O modelo está generalizando bem e não apresenta sinais significativos de overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo treinado\n",
    "model = load_model('modelo_pulmoes_vgg16_2.h5')\n",
    "\n",
    "# Carregar imagens de teste\n",
    "test_images, test_labels = load_images_from_folder(test_dir)\n",
    "\n",
    "# Codificar os rótulos de teste\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Normalizar as imagens de teste\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels_encoded, batch_size=batch_size)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "# Incluir análise dos resultados\n",
    "if (val_loss - test_loss) < 0.1 and (test_accuracy - val_accuracy) > -0.1:\n",
    "    print(\"Conclusão: O modelo está generalizando bem e não apresenta sinais significativos de overfitting.\")\n",
    "else:\n",
    "    print(\"Conclusão: Pode haver sinais de overfitting. Considere realizar ajustes adicionais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa de teste do modelo com os dados de teste. A validação é usada durante o processo de treinamento para ajustar hiperparâmetros e monitorar o desempenho em dados não vistos, enquanto o teste é a avaliação final do modelo.\n",
    "1. Carregar o modelo treinado a partir de um arquivo.\n",
    "2. Carregar e preprocessar imagens de teste de um diretório.\n",
    "3. Codificar os rótulos de teste em valores numéricos.\n",
    "4. Normalizar as imagens de teste.\n",
    "5. Avaliar o modelo usando as imagens e rótulos de teste, imprimindo a perda e a precisão.\n",
    "6. Analisar os resultados para determinar se o modelo está generalizando bem ou se há sinais de overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre o output dos dados de teste de modelo com dados de teste.\n",
    "### Análise dos Resultados ###\n",
    "Training Accuracy: 95% (estimado)  # Precisão no conjunto de treinamento (estimado)\n",
    "Training Loss: ~0.20 (estimado)  # Perda no conjunto de treinamento (estimado)\n",
    "Val Accuracy: 0.875  # Precisão no conjunto de validação\n",
    "Val Loss: 0.33996444940567017  # Perda no conjunto de validação\n",
    "Test Accuracy: 0.9054487347602844  # Precisão no conjunto de teste\n",
    "Test Loss: 0.2635897696018219  # Perda no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step\n",
      "Previsão: Pneumonia (52.10%)\n"
     ]
    }
   ],
   "source": [
    "# INFERENCIA / SIMULAÇÃO >> pode ser carregado em um arquivo ipynb novo / separado \n",
    "# Defina o tamanho da imagem usado durante o treinamento\n",
    "image_size = 224\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "model = load_model('modelo_pulmoes_vgg16_2.h5')\n",
    "\n",
    "# Função para carregar e preprocessar a imagem\n",
    "def preprocess_image(image_path, image_size):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Carregar a imagem como escala de cinza\n",
    "    image = cv2.resize(image, (image_size, image_size))  # Redimensionar para o tamanho usado no treinamento\n",
    "    image = image / 255.0  # Normalização\n",
    "    image = np.expand_dims(image, axis=-1)  # Adicionar uma dimensão extra para o canal\n",
    "    image = np.expand_dims(image, axis=0)  # Adicionar uma dimensão extra para o batch\n",
    "    return image\n",
    "\n",
    "# Função para fazer a previsão\n",
    "def make_prediction(image_path, model):\n",
    "    image = preprocess_image(image_path, image_size)\n",
    "    prediction = model.predict(image)\n",
    "    confidence = prediction[0][0]  # Pegando a confiança da previsão\n",
    "    if confidence > 0.5:\n",
    "        print(f\"Previsão: Pneumonia ({confidence * 100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"Previsão: Normal ({(1 - confidence) * 100:.2f}%)\")\n",
    "\n",
    "# Caminho para a imagem de teste >> altere aqui para testar com uma imagem de raiox local em seu computador\n",
    "image_path = 'C:/Users/andre/OneDrive/Desktop/Fiap/Kaggle/Chest_X-Ray/simulacao/person3_virus_17.jpeg'\n",
    "\n",
    "# Fazer a previsão manual\n",
    "make_prediction(image_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define o tamanho da imagem usada durante o treinamento.\n",
    "   - `image_size = 224`\n",
    "\n",
    "2. Carrega o modelo treinado a partir de um arquivo H5.\n",
    "   - `model = load_model('modelo_pulmoes_vgg16_2.h5')`\n",
    "\n",
    "3. Define uma função para carregar e pré-processar uma imagem:\n",
    "   - Carrega a imagem como escala de cinza.\n",
    "   - Redimensiona a imagem para o tamanho usado no treinamento.\n",
    "   - Normaliza a imagem.\n",
    "   - Adiciona dimensões extras para o canal e o batch.\n",
    "\n",
    "4. Define uma função para fazer a previsão com o modelo:\n",
    "   - Pré-processa a imagem.\n",
    "   - Faz a previsão usando o modelo.\n",
    "   - Calcula a confiança da previsão e imprime se é pneumonia ou normal com a porcentagem de confiança.\n",
    "\n",
    "5. Especifica o caminho para a imagem de teste.\n",
    "   - `image_path = 'C:/Users/andre/OneDrive/Desktop/Fiap/Kaggle/Chest_X-Ray/simulacao/person3_virus_17.jpeg'`\n",
    "\n",
    "6. Faz a previsão manualmente chamando a função `make_prediction`.\n",
    "   - `make_prediction(image_path, model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
